{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Loader Debug Notebook\n",
    "\n",
    "This notebook allows manual inspection and debugging of the DiffusionLoader.\n",
    "\n",
    "## What it loads:\n",
    "- AMICO-NODDI: ICVF, ISOVF, OD metrics\n",
    "- DIPY DKI: FA, MD, AD, RD, MK, AK, RK\n",
    "- DIPY MAPMRI: RTOP, RTAP, RTPP, QIV, MSD\n",
    "- DSI Studio GQI: QA, GFA, ISO\n",
    "- Auto-discovers available workflows from QSIParc directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set up paths from environment variables or override manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths from environment (or override below)\n",
    "QSIPARC_PATH = os.getenv(\"QSIPARC_PATH\")\n",
    "QSIRECON_PATH = os.getenv(\"QSIRECON_PATH\")\n",
    "SESSIONS_CSV = os.getenv(\"SESSIONS_CSV\")\n",
    "ATLAS_NAME = os.getenv(\"ATLAS_NAME\", \"4S456Parcels\")\n",
    "\n",
    "# Optionally override paths here:\n",
    "# QSIPARC_PATH = \"/path/to/qsiparc\"\n",
    "# QSIRECON_PATH = \"/path/to/qsirecon\"\n",
    "# SESSIONS_CSV = \"/path/to/sessions.csv\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  QSIPARC_PATH: {QSIPARC_PATH}\")\n",
    "print(f\"  QSIRECON_PATH: {QSIRECON_PATH}\")\n",
    "print(f\"  SESSIONS_CSV: {SESSIONS_CSV}\")\n",
    "print(f\"  ATLAS_NAME: {ATLAS_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Paths Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path, name):\n",
    "    if path is None:\n",
    "        print(f\"  {name}: NOT SET\")\n",
    "        return False\n",
    "    p = Path(path)\n",
    "    exists = p.exists()\n",
    "    print(f\"  {name}: {'EXISTS' if exists else 'MISSING'} - {p}\")\n",
    "    return exists\n",
    "\n",
    "print(\"Path verification:\")\n",
    "check_path(QSIPARC_PATH, \"QSIPARC_PATH\")\n",
    "check_path(QSIRECON_PATH, \"QSIRECON_PATH\")\n",
    "check_path(SESSIONS_CSV, \"SESSIONS_CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore QSIParc directory structure\n",
    "if QSIPARC_PATH:\n",
    "    root = Path(QSIPARC_PATH)\n",
    "    print(\"QSIParc Root Structure:\")\n",
    "    print(f\"Root: {root}\")\n",
    "    \n",
    "    if root.exists():\n",
    "        # List workflow directories\n",
    "        workflows = [d.name for d in root.iterdir() if d.is_dir() and d.name.startswith(\"qsirecon-\")]\n",
    "        print(f\"\\nDiscovered workflows: {workflows}\")\n",
    "        \n",
    "        # Look at one workflow's structure\n",
    "        if workflows:\n",
    "            wf_dir = root / workflows[0]\n",
    "            subjects = sorted([d.name for d in wf_dir.iterdir() if d.is_dir() and d.name.startswith(\"sub-\")])[:5]\n",
    "            print(f\"\\nFirst 5 subjects in {workflows[0]}: {subjects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore detailed structure of one subject/session\n",
    "if QSIPARC_PATH:\n",
    "    root = Path(QSIPARC_PATH)\n",
    "    workflows = [d.name for d in root.iterdir() if d.is_dir() and d.name.startswith(\"qsirecon-\")]\n",
    "    \n",
    "    if workflows:\n",
    "        wf_dir = root / workflows[0]\n",
    "        subjects = [d for d in wf_dir.iterdir() if d.is_dir() and d.name.startswith(\"sub-\")]\n",
    "        \n",
    "        if subjects:\n",
    "            first_sub = subjects[0]\n",
    "            sessions = [d for d in first_sub.iterdir() if d.is_dir() and d.name.startswith(\"ses-\")]\n",
    "            \n",
    "            if sessions:\n",
    "                first_ses = sessions[0]\n",
    "                print(f\"Structure of {first_sub.name}/{first_ses.name}:\")\n",
    "                \n",
    "                # Look for dwi/atlas directories\n",
    "                dwi_dir = first_ses / \"dwi\"\n",
    "                if dwi_dir.exists():\n",
    "                    for atlas_dir in dwi_dir.iterdir():\n",
    "                        if atlas_dir.is_dir():\n",
    "                            print(f\"\\n  {atlas_dir.name}/:\")\n",
    "                            for f in sorted(atlas_dir.glob(\"*.tsv\"))[:10]:\n",
    "                                print(f\"    {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Sessions CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SESSIONS_CSV and Path(SESSIONS_CSV).exists():\n",
    "    sessions = pd.read_csv(SESSIONS_CSV, dtype={\"subject_code\": str, \"session_id\": str})\n",
    "    print(f\"Sessions CSV loaded: {len(sessions)} rows\")\n",
    "    print(f\"\\nColumns: {list(sessions.columns)}\")\n",
    "    print(f\"\\nFirst 10 sessions:\")\n",
    "    display(sessions.head(10))\n",
    "else:\n",
    "    print(\"Sessions CSV not found\")\n",
    "    sessions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize DiffusionLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroalign_preprocessing.loaders import DiffusionLoader\n",
    "\n",
    "loader = DiffusionLoader(\n",
    "    qsiparc_path=QSIPARC_PATH,\n",
    "    qsirecon_path=QSIRECON_PATH,\n",
    "    workflows=None,  # Auto-discover all workflows\n",
    "    atlas_name=ATLAS_NAME,\n",
    "    n_jobs=1  # Use serial for debugging\n",
    ")\n",
    "\n",
    "print(f\"DiffusionLoader initialized:\")\n",
    "print(f\"  qsiparc_path: {loader.paths.qsiparc_path}\")\n",
    "print(f\"  qsirecon_path: {loader.paths.qsirecon_path}\")\n",
    "print(f\"  atlas_name: {loader.paths.atlas_name}\")\n",
    "print(f\"  Discovered workflows: {loader.workflows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Single Session Loading\n",
    "\n",
    "Load a single session to inspect the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first session from the CSV\n",
    "if sessions is not None and len(sessions) > 0:\n",
    "    test_subject = sessions.iloc[0][\"subject_code\"]\n",
    "    test_session = sessions.iloc[0][\"session_id\"]\n",
    "    print(f\"Testing with: sub-{test_subject}_ses-{test_session}\")\n",
    "else:\n",
    "    # Manual override if no sessions CSV\n",
    "    test_subject = \"001\"\n",
    "    test_session = \"01\"\n",
    "    print(f\"Using manual test subject: sub-{test_subject}_ses-{test_session}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which workflows have data for this session\n",
    "print(f\"Session directory check for sub-{test_subject}_ses-{test_session}:\")\n",
    "for wf in loader.workflows:\n",
    "    session_dir = loader.get_session_directory(test_subject, test_session, wf)\n",
    "    if session_dir:\n",
    "        n_files = len(list(session_dir.glob(\"*_parc.tsv\")))\n",
    "        print(f\"  {wf}: EXISTS ({n_files} TSV files)\")\n",
    "    else:\n",
    "        print(f\"  {wf}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect TSV files in one workflow\n",
    "if loader.workflows:\n",
    "    test_workflow = loader.workflows[0]\n",
    "    session_dir = loader.get_session_directory(test_subject, test_session, test_workflow)\n",
    "    \n",
    "    if session_dir:\n",
    "        print(f\"TSV files in {test_workflow}:\")\n",
    "        for f in sorted(session_dir.glob(\"*_parc.tsv\")):\n",
    "            print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load single session (all workflows)\n",
    "single_session_df = loader.load_session(\n",
    "    subject=test_subject,\n",
    "    session=test_session,\n",
    "    workflow=None,  # Load all workflows\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "if single_session_df is not None:\n",
    "    print(f\"Single session loaded: {len(single_session_df)} rows\")\n",
    "    print(f\"\\nColumns: {list(single_session_df.columns)}\")\n",
    "    print(f\"\\nWorkflows: {single_session_df['workflow'].unique().tolist()}\")\n",
    "    print(f\"Models: {single_session_df['model'].unique().tolist()}\")\n",
    "    print(f\"Params: {single_session_df['param'].unique().tolist()}\")\n",
    "else:\n",
    "    print(\"Failed to load session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data for each workflow/model combination\n",
    "if single_session_df is not None:\n",
    "    for workflow in single_session_df['workflow'].unique():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"WORKFLOW: {workflow}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        wf_df = single_session_df[single_session_df['workflow'] == workflow]\n",
    "        \n",
    "        # Show model/param combinations\n",
    "        combos = wf_df[['model', 'param']].drop_duplicates()\n",
    "        print(f\"Model/Param combinations:\")\n",
    "        for _, row in combos.iterrows():\n",
    "            print(f\"  {row['model']} - {row['param']}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"\\nSample data:\")\n",
    "        display(wf_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inspect Raw TSV Files\n",
    "\n",
    "Look at the raw TSV files to verify data integrity and understand the BIDS naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroalign_preprocessing.loaders.diffusion import parse_bids_entities\n",
    "\n",
    "# Load and display raw TSV files from first workflow\n",
    "if loader.workflows:\n",
    "    test_workflow = loader.workflows[0]\n",
    "    session_dir = loader.get_session_directory(test_subject, test_session, test_workflow)\n",
    "    \n",
    "    if session_dir:\n",
    "        tsv_files = sorted(session_dir.glob(\"*_parc.tsv\"))[:5]  # First 5 files\n",
    "        \n",
    "        for tsv_file in tsv_files:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"RAW TSV: {tsv_file.name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Parse BIDS entities\n",
    "            entities = parse_bids_entities(tsv_file.name)\n",
    "            print(f\"Parsed entities: {entities}\")\n",
    "            \n",
    "            # Load and display\n",
    "            raw_df = pd.read_csv(tsv_file, sep=\"\\t\")\n",
    "            print(f\"Shape: {raw_df.shape}\")\n",
    "            print(f\"Columns: {list(raw_df.columns)}\")\n",
    "            display(raw_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Multiple Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small batch of sessions for testing\n",
    "if sessions is not None:\n",
    "    # Create a temporary CSV with first N sessions\n",
    "    n_test = min(5, len(sessions))\n",
    "    test_sessions = sessions.head(n_test)\n",
    "    temp_csv = Path(\"/tmp/test_sessions.csv\")\n",
    "    test_sessions.to_csv(temp_csv, index=False)\n",
    "    \n",
    "    print(f\"Testing batch load with {n_test} sessions...\")\n",
    "    \n",
    "    batch_df = loader.load_sessions(\n",
    "        sessions_csv=temp_csv,\n",
    "        workflow=None,  # All workflows\n",
    "        n_jobs=1,\n",
    "        progress=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBatch loaded: {len(batch_df)} rows\")\n",
    "    print(f\"Unique sessions: {batch_df[['subject_code', 'session_id']].drop_duplicates().shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "if 'batch_df' in dir() and batch_df is not None:\n",
    "    print(\"Summary by workflow/model/param:\")\n",
    "    summary = batch_df.groupby(['workflow', 'model', 'param']).agg({\n",
    "        'label': 'nunique',\n",
    "        'subject_code': 'nunique'\n",
    "    }).rename(columns={'label': 'n_regions', 'subject_code': 'n_subjects'})\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the get_available_parameters method\n",
    "if 'batch_df' in dir() and batch_df is not None:\n",
    "    params_by_model = loader.get_available_parameters(batch_df)\n",
    "    print(\"Available parameters by model:\")\n",
    "    for model, params in params_by_model.items():\n",
    "        print(f\"  {model}: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'batch_df' in dir() and batch_df is not None:\n",
    "    print(\"Data Quality Checks:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\n1. Missing values per column:\")\n",
    "    missing = batch_df.isnull().sum()\n",
    "    missing_pct = (missing / len(batch_df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({'missing': missing, 'pct': missing_pct})\n",
    "    display(missing_df[missing_df['missing'] > 0])\n",
    "    \n",
    "    # Check for negative values in metric columns\n",
    "    print(\"\\n2. Negative values check:\")\n",
    "    numeric_cols = batch_df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        neg_count = (batch_df[col] < 0).sum()\n",
    "        if neg_count > 0:\n",
    "            print(f\"  WARNING: {col} has {neg_count} negative values\")\n",
    "    print(\"  Done (no warnings = good)\")\n",
    "    \n",
    "    # Check for duplicate entries\n",
    "    print(\"\\n3. Duplicate check:\")\n",
    "    dup_cols = ['subject_code', 'session_id', 'workflow', 'model', 'param', 'label']\n",
    "    dups = batch_df.duplicated(subset=dup_cols, keep=False).sum()\n",
    "    print(f\"  Duplicate rows: {dups}\")\n",
    "    \n",
    "    # Check region counts are consistent\n",
    "    print(\"\\n4. Region count consistency:\")\n",
    "    region_counts = batch_df.groupby(['subject_code', 'session_id', 'workflow', 'model', 'param'])['label'].nunique()\n",
    "    print(f\"  Regions per workflow/model/param:\")\n",
    "    display(region_counts.groupby(['workflow', 'model', 'param']).agg(['min', 'max', 'mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value ranges for common diffusion metrics\n",
    "if 'batch_df' in dir() and batch_df is not None:\n",
    "    print(\"\\n5. Value range checks for diffusion metrics:\")\n",
    "    \n",
    "    # Expected ranges (approximate)\n",
    "    expected_ranges = {\n",
    "        'FA': (0, 1),      # Fractional Anisotropy\n",
    "        'MD': (0, 0.005),  # Mean Diffusivity (mm^2/s)\n",
    "        'ICVF': (0, 1),    # Intracellular Volume Fraction (NODDI)\n",
    "        'ISOVF': (0, 1),   # Isotropic Volume Fraction (NODDI)\n",
    "        'OD': (0, 1),      # Orientation Dispersion (NODDI)\n",
    "    }\n",
    "    \n",
    "    for param, (exp_min, exp_max) in expected_ranges.items():\n",
    "        param_df = batch_df[batch_df['param'] == param]\n",
    "        if len(param_df) > 0 and 'mean' in param_df.columns:\n",
    "            actual_min = param_df['mean'].min()\n",
    "            actual_max = param_df['mean'].max()\n",
    "            in_range = actual_min >= exp_min and actual_max <= exp_max\n",
    "            status = \"OK\" if in_range else \"WARNING\"\n",
    "            print(f\"  {param}: [{actual_min:.4f}, {actual_max:.4f}] expected [{exp_min}, {exp_max}] - {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of a diffusion metric\n",
    "if 'batch_df' in dir() and batch_df is not None:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Pick a common metric (FA if available)\n",
    "    for metric in ['FA', 'ICVF', 'MD']:\n",
    "        metric_df = batch_df[batch_df['param'] == metric]\n",
    "        if len(metric_df) > 0 and 'mean' in metric_df.columns:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "            metric_df['mean'].hist(bins=50, ax=ax)\n",
    "            ax.set_xlabel('Mean Value')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'Distribution of {metric} Regional Means')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Specific Workflow Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only a specific workflow\n",
    "if loader.workflows and sessions is not None:\n",
    "    test_workflow = loader.workflows[0]\n",
    "    print(f\"Loading only workflow: {test_workflow}\")\n",
    "    \n",
    "    n_test = min(3, len(sessions))\n",
    "    test_sessions = sessions.head(n_test)\n",
    "    temp_csv = Path(\"/tmp/test_sessions.csv\")\n",
    "    test_sessions.to_csv(temp_csv, index=False)\n",
    "    \n",
    "    workflow_df = loader.load_sessions(\n",
    "        sessions_csv=temp_csv,\n",
    "        workflow=test_workflow,\n",
    "        n_jobs=1,\n",
    "        progress=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nLoaded {len(workflow_df)} rows\")\n",
    "    print(f\"Workflows in result: {workflow_df['workflow'].unique().tolist()}\")\n",
    "    print(f\"Models: {workflow_df['model'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Debug Specific Issues\n",
    "\n",
    "Use this section to debug specific issues you encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug a specific subject/session\n",
    "debug_subject = \"\"  # Fill in subject code\n",
    "debug_session = \"\"  # Fill in session ID\n",
    "debug_workflow = \"\"  # Fill in workflow name (optional)\n",
    "\n",
    "if debug_subject and debug_session:\n",
    "    print(f\"Debugging: sub-{debug_subject}_ses-{debug_session}\")\n",
    "    \n",
    "    # Check paths for all workflows\n",
    "    for wf in loader.workflows:\n",
    "        sess_dir = loader.get_session_directory(debug_subject, debug_session, wf)\n",
    "        if sess_dir:\n",
    "            print(f\"\\n{wf}: {sess_dir}\")\n",
    "            print(f\"  Files:\")\n",
    "            for f in sorted(sess_dir.glob(\"*.tsv\"))[:5]:\n",
    "                print(f\"    {f.name}\")\n",
    "    \n",
    "    # Try loading\n",
    "    debug_df = loader.load_session(\n",
    "        debug_subject, \n",
    "        debug_session,\n",
    "        workflow=debug_workflow if debug_workflow else None\n",
    "    )\n",
    "    if debug_df is not None:\n",
    "        print(f\"\\nLoaded {len(debug_df)} rows\")\n",
    "        display(debug_df.head())\n",
    "    else:\n",
    "        print(\"\\nFailed to load session data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
